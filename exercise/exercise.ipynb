{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSC2611 Exercise: Meaning construction from text\n",
    "\n",
    "Self-assessment/precursor exercise to assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all libraries\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import errstate\n",
    "\n",
    "#Q1\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for formatting\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Extract the 5000 most common English words (denoted by W) based on unigram frequencies in the Brown corpus.  Report the 5 most and least common words you have found in the 5000 words.  Update W by adding n words where n is the set of words in Table 1of RG65 that were not included in the top 5000 words from the Brown corpus.  Denote the total number of words in W as |W|.\n",
    "\n",
    "Note: Including stopwords in the English corpus, but removing punctuations such as \"`\" present in the corpus + lowercasing all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all Brown words; brown.words() returns an iterator\n",
    "words_iter = brown.words()\n",
    "brown_words = [word.lower() for word in words_iter if (word not in string.punctuation)]\n",
    "\n",
    "# getting unigram counts\n",
    "uni_counts = Counter(brown_words)\n",
    "\n",
    "# getting most common 5000\n",
    "top_5000 = uni_counts.most_common(5000)\n",
    "\n",
    "# top 5000 words\n",
    "W = [word for (word, count) in top_5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common in top 5000 [('the', 69971), ('of', 36412), ('and', 28853), ('to', 26158), ('a', 23195)]\n",
      "5 least common in top 5000 [('input', 20), ('suitcase', 20), ('skyros', 20), ('freddy', 20), ('airport', 19)]\n"
     ]
    }
   ],
   "source": [
    "# Top 500 words\n",
    "print(\"5 most common in top 5000\", top_5000[:5])\n",
    "print(\"5 least common in top 5000\", top_5000[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adding words from RG65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|W|= 5021\n"
     ]
    }
   ],
   "source": [
    "rg_words_first = [\n",
    "    \"cord\",\n",
    "    \"hill\",\n",
    "    \"rooster\",\n",
    "    \"car\",\n",
    "    \"noon\",\n",
    "    \"cemetery\",\n",
    "    \"fruit\",\n",
    "    \"glass\",\n",
    "    \"autograph\",\n",
    "    \"magician\",\n",
    "    \"automobile\",\n",
    "    \"crane\",\n",
    "    \"mound\",\n",
    "    \"brother\",\n",
    "    \"grin\",\n",
    "    \"sage\",\n",
    "    \"asylum\",\n",
    "    \"oracle\",\n",
    "    \"asylum\",\n",
    "    \"bird\",\n",
    "    \"graveyard\",\n",
    "    \"bird\",\n",
    "    \"glass\",\n",
    "    \"food\",\n",
    "    \"boy\",\n",
    "    \"brother\",\n",
    "    \"cushion\",\n",
    "    \"asylum\",\n",
    "    \"monk\",\n",
    "    \"furnace\",\n",
    "    \"asylum\",\n",
    "    \"magician\",\n",
    "    \"coast\",\n",
    "    \"hill\",\n",
    "    \"grin\",\n",
    "    \"cord\",\n",
    "    \"shore\",\n",
    "    \"glass\",\n",
    "    \"monk\",\n",
    "    \"grin\",\n",
    "    \"boy\",\n",
    "    \"serf\",\n",
    "    \"automobile\",\n",
    "    \"journey\",\n",
    "    \"mound\",\n",
    "    \"autograph\",\n",
    "    \"lad\",\n",
    "    \"coast\",\n",
    "    \"forest\",\n",
    "    \"forest\",\n",
    "    \"food\",\n",
    "    \"implement\",\n",
    "    \"cemetery\",\n",
    "    \"cock\",\n",
    "    \"shore\",\n",
    "    \"boy\",\n",
    "    \"bird\",\n",
    "    \"cushion\",\n",
    "    \"coast\",\n",
    "    \"cemetery\",\n",
    "    \"furnace\",\n",
    "    \"automobile\",\n",
    "    \"crane\",\n",
    "    \"midday\",\n",
    "    \"gem\",\n",
    "]\n",
    "rg_words_second = [\n",
    "    \"smile\",\n",
    "    \"woodland\",\n",
    "    \"voyage\",\n",
    "    \"journey\",\n",
    "    \"string\",\n",
    "    \"mound\",\n",
    "    \"furnace\",\n",
    "    \"jewel\",\n",
    "    \"shore\",\n",
    "    \"oracle\",\n",
    "    \"wizard\",\n",
    "    \"implement\",\n",
    "    \"stove\",\n",
    "    \"lad\",\n",
    "    \"implement\",\n",
    "    \"wizard\",\n",
    "    \"fruit\",\n",
    "    \"sage\",\n",
    "    \"monk\",\n",
    "    \"crane\",\n",
    "    \"madhouse\",\n",
    "    \"cock\",\n",
    "    \"magician\",\n",
    "    \"fruit\",\n",
    "    \"rooster\",\n",
    "    \"monk\",\n",
    "    \"jewel\",\n",
    "    \"madhouse\",\n",
    "    \"slave\",\n",
    "    \"stove\",\n",
    "    \"cemetery\",\n",
    "    \"wizard\",\n",
    "    \"forest\",\n",
    "    \"mound\",\n",
    "    \"lad\",\n",
    "    \"string\",\n",
    "    \"woodland\",\n",
    "    \"tumbler\",\n",
    "    \"oracle\",\n",
    "    \"smile\",\n",
    "    \"sage\",\n",
    "    \"slave\",\n",
    "    \"cushion\",\n",
    "    \"voyage\",\n",
    "    \"shore\",\n",
    "    \"signature\",\n",
    "    \"wizard\",\n",
    "    \"shore\",\n",
    "    \"graveyard\",\n",
    "    \"woodland\",\n",
    "    \"rooster\",\n",
    "    \"tool\",\n",
    "    \"woodland\",\n",
    "    \"rooster\",\n",
    "    \"voyage\",\n",
    "    \"lad\",\n",
    "    \"woodland\",\n",
    "    \"pillow\",\n",
    "    \"hill\",\n",
    "    \"graveyard\",\n",
    "    \"implement\",\n",
    "    \"car\",\n",
    "    \"rooster\",\n",
    "    \"noon\",\n",
    "    \"jewel\",\n",
    "]\n",
    "sim_scores = [\n",
    "    0.02,\n",
    "    1.48,\n",
    "    0.04,\n",
    "    1.55,\n",
    "    0.04,\n",
    "    1.69,\n",
    "    0.05,\n",
    "    1.78,\n",
    "    0.06,\n",
    "    1.82,\n",
    "    0.11,\n",
    "    2.37,\n",
    "    0.14,\n",
    "    2.41,\n",
    "    0.18,\n",
    "    2.46,\n",
    "    0.19,\n",
    "    2.61,\n",
    "    0.39,\n",
    "    2.63,\n",
    "    0.42,\n",
    "    2.63,\n",
    "    0.44,\n",
    "    2.69,\n",
    "    0.44,\n",
    "    2.74,\n",
    "    0.45,\n",
    "    3.04,\n",
    "    0.57,\n",
    "    3.11,\n",
    "    0.79,\n",
    "    3.21,\n",
    "    0.85,\n",
    "    3.29,\n",
    "    0.88,\n",
    "    3.41,\n",
    "    0.9,\n",
    "    3.45,\n",
    "    0.91,\n",
    "    3.46,\n",
    "    0.96,\n",
    "    3.46,\n",
    "    0.97,\n",
    "    3.58,\n",
    "    0.97,\n",
    "    3.59,\n",
    "    0.99,\n",
    "    3.6,\n",
    "    1.0,\n",
    "    3.65,\n",
    "    1.09,\n",
    "    3.66,\n",
    "    1.18,\n",
    "    3.68,\n",
    "    1.22,\n",
    "    3.82,\n",
    "    1.24,\n",
    "    3.84,\n",
    "    1.26,\n",
    "    3.88,\n",
    "    1.37,\n",
    "    3.92,\n",
    "    1.41,\n",
    "    3.94,\n",
    "    3.94,\n",
    "]\n",
    "\n",
    "\n",
    "rg_all_words = []\n",
    "rg_all_words.extend(rg_words_first)\n",
    "rg_all_words.extend(rg_words_second)\n",
    "rg_all_words = list(set(rg_words_first))\n",
    "\n",
    "# assert all lists have same number of values, i.e., 65\n",
    "assert len(rg_words_first)==len(rg_words_second)==len(sim_scores)==65\n",
    "\n",
    "\n",
    "# getting number of words not in n\n",
    "n = len(set(rg_all_words)) - len(set(rg_all_words).intersection(W))\n",
    "\n",
    "# getting words not in W already, but present in RG65 words\n",
    "new_rg_words = list(set(rg_all_words) - set(rg_all_words).intersection(W))\n",
    "\n",
    "W.extend(new_rg_words)\n",
    "\n",
    "mod_W = len(W)\n",
    "print(\"|W|=\", mod_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Construct a word-context vector model (denoted by M1) by collecting bigram counts for words in W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get bigrams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# to build a sparse matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = list(ngrams(words_iter, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a mapping from words to indices, or vocabulary IDs\n",
    "vocab_to_id = {}\n",
    "for i, word in enumerate(W):\n",
    "    vocab_to_id[word] = i\n",
    "\n",
    "# Saving the dictionary for future exercise\n",
    "output = open(\"../lab/data/lsa_vocab_to_id.pkl\", \"wb\")\n",
    "pickle.dump(vocab_to_id, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing M1\n",
    "# |W|x|W| matrix\n",
    "M1 = np.zeros((mod_W, mod_W))\n",
    "\n",
    "for (first_word, second_word) in bigram_list:\n",
    "    try:\n",
    "        M1[vocab_to_id[first_word]][vocab_to_id[second_word]] += 1\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "M1 = sparse.csr_matrix(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ppmi\n",
    "import math\n",
    "def compute_ppmi(M1):\n",
    "    normalization_constant = np.sum(M1)\n",
    "    pmi_before_log = M1.copy()\n",
    "    \n",
    "    p_i = np.sum(M1,axis=0)/normalization_constant\n",
    "    p_j = np.sum(M1,axis=1)/normalization_constant\n",
    "    \n",
    "    # p(i)*p(j)\n",
    "    p_i_into_p_j = np.outer(p_i,p_j)\n",
    "    \n",
    "    div_factor = p_i_into_p_j\n",
    "    pmi_before_log = M1/(p_i_into_p_j*normalization_constant)\n",
    "\n",
    "    # setting NaN to 0\n",
    "    pmi_before_log[np.isnan(pmi_before_log)] = 0\n",
    "    \n",
    "    # only considering positive infinity since all\n",
    "    # values in matrix are positive frequencies\n",
    "    pmi_before_log[~np.isfinite(pmi_before_log)] = 1.7976931348623157e+308\n",
    "\n",
    "    with errstate(divide='ignore'):\n",
    "        pmi = np.log(pmi_before_log)\n",
    "\n",
    "    ppmi = pmi.copy()\n",
    "\n",
    "    # returning POSITIVE PMI\n",
    "    ppmi[ppmi < 0] = 0.0\n",
    "\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_plus = compute_ppmi(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 2.7147188 , 2.20389317,\n",
       "         0.        ],\n",
       "        [1.6717954 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.90421793, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 1.82589416, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [1.2899834 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Construct  a  latent  semantic  model using PCA with varying dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_10 = PCA(n_components=10)\n",
    "lsa_100 = PCA(n_components=100)\n",
    "lsa_300 = PCA(n_components=300)\n",
    "\n",
    "M2_10 = lsa_10.fit_transform(M1_plus)\n",
    "M2_100 = lsa_100.fit_transform(M1_plus)\n",
    "M2_300 = lsa_300.fit_transform(M1_plus)\n",
    "\n",
    "# Saving the dictionary for future exercise\n",
    "output = open(\"../lab/data/lsa_300.pkl\", \"wb\")\n",
    "pickle.dump(M2_300, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Find all pairs of words in Table 1 of RG65 that are also available in W.  Denote these pairs as P.  Record the human-judged similarities of these word pairs from the table and denote similarity values as S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all word-pairs such that both words in pair are in W\n",
    "\n",
    "P = []\n",
    "S = []\n",
    "count = 0\n",
    "for first_word, second_word in zip(rg_words_first, rg_words_second):\n",
    "    if (first_word in W) and (second_word in W):\n",
    "        P.append((first_word, second_word))\n",
    "        S.append(sim_scores[count])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Perform the following calculations on each of these models M1,M1+, M210, M2100,M2300, separately:  Calculate cosine similarity between each pair of words in P, based on the constructed word vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to comput similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1 = []\n",
    "sm1_plus = []\n",
    "sm2_10 = []\n",
    "sm2_100 = []\n",
    "sm2_300 = []\n",
    "for i, (first_word, second_word) in enumerate(P):\n",
    "    first_id = vocab_to_id[first_word]\n",
    "    second_id = vocab_to_id[second_word]\n",
    "\n",
    "    sm1.append(cosine_similarity(M1[first_id], M1[second_id])[0][0])\n",
    "    sm1_plus.append(cosine_similarity(M1_plus[first_id], M1_plus[second_id])[0][0])\n",
    "    sm2_10.append(cosine_similarity([M2_10[first_id]], [M2_10[second_id]])[0][0])\n",
    "    sm2_100.append(cosine_similarity([M2_100[first_id]], [M2_100[second_id]])[0][0])\n",
    "    sm2_300.append(cosine_similarity([M2_300[first_id]], [M2_300[second_id]])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM1 [0.0, 0.38919299029595256, 0.0, 0.31430927854685603, 0.0, 0.0, 0.5, 0.1770844008302866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12700012700019048, 0.39466488146729556, 0.0, 0.049386479832479485, 0.048795003647426664, 0.0, 0.3571973786082756, 0.0, 0.0, 0.0, 0.4226664576955308, 0.0, 0.0, 0.0, 0.20025046972870353, 0.27773983045116846, 0.0, 0.0, 0.0, 0.0, 0.3684529491774706, 0.10088665446106368, 0.2886751345948129, 0.0, 0.4474063620329013, 0.0, 0.26726124191242434]\n",
      "********************\n",
      "SM1_plus [0.0, 0.05846616937858707, 0.0, 0.03052650650178733, 0.0, 0.0, 0.37271372107998707, 0.06799305472679423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004768327526323783, 0.06789362325137147, 0.0, 0.0, 0.035824001987207525, 0.0, 0.09051163400453825, 0.0, 0.0, 0.0, 0.12309439071203251, 0.0, 0.0, 0.0, 0.0026153643644516, 0.007467790041795627, 0.0, 0.0, 0.0, 0.0, 0.07486335267152128, 0.009753530236653884, 0.05612817129282398, 0.0, 0.03741200839645414, 0.0, 0.06451776254201196]\n",
      "********************\n",
      "SM2_10 [-0.11533849471570708, 0.34291593348986027, 0.4862639180256207, -0.011928467130775937, 0.8899454257253239, 0.9999884138340429, 0.7781012647541774, 0.999650677654967, -0.007162296068429139, -0.01198041535744809, 0.9999101471903159, 0.844804724306354, -0.009698916370236215, -0.4087595309848847, 0.4648735303191763, -0.014865551632553542, 0.627455967838095, 0.8429097610366403, 0.6203227265918729, 0.9999454542439216, -0.013240844030726175, -0.16168939967458335, 0.5621766269860655, 0.845408202060526, -0.12380248534243182, 0.6254331478501182, 0.633097505803406, 0.08267685676913109, 0.1347777737030334, 0.03963187871065631, -0.2906595250700645, 0.21326221860131991, 0.3169202444595221, -0.0808516642255512, 0.6266001532080854, -0.17069766749769844, 0.9999060800028273, 0.75158626150073, 0.942634938671607, 0.17198277168725393, 0.9912299934904772]\n",
      "********************\n",
      "SM2_100 [-0.18300585278458542, 0.12105586278477876, 0.03619971950832854, 0.10110366639208687, 0.10940822996117051, 0.42876778743628585, 0.7039909641797023, 0.19560335692076616, 0.035416661295254934, 0.12465684025462756, 0.44457539196607937, 0.3092558651650765, -0.063094162966618, 0.011701081259674305, 0.11253068701899309, 0.18618857516408183, -0.11818796057074965, 0.375736222565335, 0.185971960423168, 0.1708724263623831, 0.2285297029973698, -0.08022984991211678, 0.021231970522422615, 0.13509747532381977, 0.1726471947121144, -0.1332809201749639, 0.34672360926641155, 0.09815302512350717, -0.0521774507745351, -0.09586188066762898, -0.0760110872748899, -0.1899659601684283, -0.013379282656445671, 0.5991031525659345, 0.24781298583408484, -0.040079211951450484, 0.17126508045881766, 0.1584591498345436, 0.09649112906370383, -0.11792004348200151, 0.7553261334015383]\n",
      "********************\n",
      "SM2_300 [-0.08397621772046517, 0.0904717462930661, -0.010941561083016997, 0.02387730043410547, 0.07281093652735923, 0.17558234952178517, 0.5111078847074388, 0.14326090570055341, 0.05576483639888487, 0.06568590211140526, 0.22510562738135445, 0.18497584166216877, -0.03741663613500119, -0.0030497155711455546, -0.006377164582759366, 0.11520229360536016, -0.10767512259861987, 0.10240776647682627, 0.17868792023691607, 0.16913596563846886, 0.25860862064919593, -0.0293055337995405, 0.0374498476771972, 0.05467350747626128, 0.13649515831762832, -0.08889628692015264, 0.1491277713035112, -0.01685581335271885, -0.02799446088241137, -0.036449679310837084, -0.028455191736994404, -0.09081738631010396, -0.02310790608799986, 0.5311255358367057, 0.14018895199462222, -0.030802293157999455, 0.10066880332258353, 0.05862899195573258, 0.09124929553428629, -0.11524080185640778, 0.5031835876336856]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "sim_names = [\"SM1\", \"SM1_plus\", \"SM2_10\", \"SM2_100\", \"SM2_300\"]\n",
    "sim_arrs = [sm1, sm1_plus, sm2_10, sm2_100, sm2_300]\n",
    "for sim_name, sim_arr in zip(sim_names, sim_arrs):\n",
    "    print(sim_name, sim_arr)\n",
    "    print(\"*\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Report Pearson correlation between S and each of the model-predicted similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between S and SM1 is 0.383297471392835, with p-value 0.013372475262142703\n",
      "Pearson correlation between S and SM1_plus is 0.23415928136700334, with p-value 0.1406007563000264\n",
      "Pearson correlation between S and SM2_10 is 0.10749842758503675, with p-value 0.5035071277381504\n",
      "Pearson correlation between S and SM2_100 is 0.36970425087090264, with p-value 0.017358390444229956\n",
      "Pearson correlation between S and SM2_300 is 0.35839609577091425, with p-value 0.0213966431652426\n"
     ]
    }
   ],
   "source": [
    "sim_names = [\"SM1\", \"SM1_plus\", \"SM2_10\", \"SM2_100\", \"SM2_300\"]\n",
    "sim_arrs = [sm1, sm1_plus, sm2_10, sm2_100, sm2_300]\n",
    "\n",
    "for sim_name, sim_arr in zip(sim_names, sim_arrs):\n",
    "    stat, p = pearsonr(S, sim_arr)\n",
    "    print(\n",
    "        \"Pearson correlation between S and {} is {}, with p-value {}\".format(\n",
    "            sim_name, stat, p\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
